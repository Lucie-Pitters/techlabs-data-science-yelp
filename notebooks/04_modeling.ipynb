{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modeling \n",
    "This notebook contains our machine learning algorithm (Naive Bayes: partially implemented, logistic regression: not implemented yet, ...). It builds on the preprocessing steps (bag-of-words and tf-idf). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Text processing\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Machine learning\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    cross_val_score,\n",
    "    StratifiedKFold,\n",
    "    GridSearchCV,\n",
    ")\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- File Paths Setup ---\n",
    "# Base directory (go 2 levels up from /src1/)\n",
    "base_dir = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "\n",
    "# Define input/output directories using relative paths\n",
    "input_dir = os.path.join(base_dir, \"data\", \"intermediate\")\n",
    "output_dir = os.path.join(base_dir, \"data\", \"intermediate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Lemmatizer & Sentiment Analyzer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Function for Text Preprocessing (Lemmatization & Cleaning)\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    text = re.sub(r\"\\d+\", \"\", text)\n",
    "    text = re.sub(r\"[^a-z\\s]\", \"\", text)\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return \" \".join(lemmatized_tokens)\n",
    "\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_json(os.path.join(input_dir, \"cleaned_reviews_2021-01.json\"), lines=True)\n",
    "\n",
    "# Apply text preprocessing\n",
    "df[\"clean_text\"] = df[\"text\"].apply(preprocess_text)\n",
    "\n",
    "# Define Multi-Class Sentiment Labels\n",
    "df[\"sentiment\"] = df[\"stars\"].apply(\n",
    "    lambda x: 0 if x <= 2 else (1 if x == 3 else (2 if x == 4 else 3))\n",
    ")\n",
    "\n",
    "# Define Custom Stopwords (Convert set to list!)\n",
    "custom_stopwords = [\n",
    "    \"restaurant\",\n",
    "    \"food\",\n",
    "    \"place\",\n",
    "    \"service\",\n",
    "    \"menu\",\n",
    "    \"great\",\n",
    "    \"good\",\n",
    "    \"bad\",\n",
    "    \"nice\",\n",
    "    \"love\",\n",
    "    \"best\",\n",
    "]\n",
    "\n",
    "# TF-IDF Vectorization with Optimized Parameters\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=400000,\n",
    "    sublinear_tf=True,\n",
    "    max_df=0.5,\n",
    "    min_df=3,\n",
    "    ngram_range=(1, 3),\n",
    "    stop_words=custom_stopwords,\n",
    "    norm=\"l2\",\n",
    ")\n",
    "\n",
    "X_tfidf = vectorizer.fit_transform(df[\"clean_text\"])\n",
    "\n",
    "# Generate Additional Features\n",
    "df[\"review_length\"] = df[\"clean_text\"].apply(lambda x: len(x.split()))\n",
    "df[\"num_exclamation\"] = df[\"text\"].apply(lambda x: x.count(\"!\"))\n",
    "df[\"avg_word_length\"] = df[\"clean_text\"].apply(\n",
    "    lambda x: np.mean([len(word) for word in x.split()]) if x.split() else 0\n",
    ")\n",
    "\n",
    "# Sentiment Scores (Using VADER)\n",
    "df[\"sentiment_pos\"] = df[\"text\"].apply(lambda x: sia.polarity_scores(x)[\"pos\"])\n",
    "df[\"sentiment_neg\"] = df[\"text\"].apply(lambda x: sia.polarity_scores(x)[\"neg\"])\n",
    "df[\"sentiment_neu\"] = df[\"text\"].apply(lambda x: sia.polarity_scores(x)[\"neu\"])\n",
    "\n",
    "# Convert additional features to NumPy array\n",
    "X_additional = np.array(\n",
    "    df[\n",
    "        [\n",
    "            \"review_length\",\n",
    "            \"num_exclamation\",\n",
    "            \"avg_word_length\",\n",
    "            \"sentiment_pos\",\n",
    "            \"sentiment_neg\",\n",
    "            \"sentiment_neu\",\n",
    "        ]\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Combine TF-IDF with additional features\n",
    "X_final = hstack((X_tfidf, X_additional))\n",
    "y = df[\"sentiment\"].values\n",
    "\n",
    "# Handle Class Imbalance Using SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_final, y)\n",
    "\n",
    "# Compute Class Weights to Adjust for Remaining Imbalance\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight=\"balanced\", classes=np.unique(y_resampled), y=y_resampled\n",
    ")\n",
    "class_weight_dict = dict(zip(np.unique(y_resampled), class_weights))\n",
    "\n",
    "# Split Data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_resampled, y_resampled, test_size=0.2, stratify=y_resampled, random_state=42\n",
    ")\n",
    "\n",
    "# Optimize Naive Bayes Model with Grid Search\n",
    "param_grid = {\"alpha\": np.linspace(0.001, 2.0, 20)}\n",
    "grid_search = GridSearchCV(\n",
    "    ComplementNB(), param_grid, cv=10, scoring=\"accuracy\", n_jobs=-1\n",
    ")\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_alpha = grid_search.best_params_[\"alpha\"]\n",
    "\n",
    "# Train Optimized Naive Bayes Model with Class Weights\n",
    "nb_classifier = ComplementNB(alpha=best_alpha)\n",
    "nb_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make Predictions\n",
    "y_pred = nb_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate Performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"âœ… Accuracy: {accuracy:.4f}\")\n",
    "print(\n",
    "    classification_report(\n",
    "        y_test,\n",
    "        y_pred,\n",
    "        target_names=[\"Negative\", \"Neutral\", \"Positive\", \"Very Positive\"],\n",
    "    )\n",
    ")\n",
    "\n",
    "# Perform Stratified Cross-Validation\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "cv_scores = cross_val_score(nb_classifier, X_resampled, y_resampled, cv=skf, n_jobs=-1)\n",
    "print(f\" Stratified Cross-validation accuracy: {cv_scores.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multinomial Logistic Regression algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "file_path = os.path.join(input_dir, \"bow_vector.csv\")\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Assume the first column is the text reviews (not used in ML model) and the second column is \"sentiment label\"\n",
    "X = df.iloc[:, 2:]  # Features (numerical representation of reviews)\n",
    "y = df.iloc[:, 1]  # Target (sentiment label)\n",
    "\n",
    "# Split into train-test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Train Multinomial Logistic Regression\n",
    "model = LogisticRegression(multi_class=\"multinomial\", solver=\"lbfgs\", max_iter=500)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Classification report\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
