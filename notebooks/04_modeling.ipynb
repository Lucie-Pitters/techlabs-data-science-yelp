{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modeling \n",
    "This notebook contains our machine learning algorithm (Naive Bayes: partially implemented, logistic regression: not implemented yet, ...). It builds on the preprocessing steps (bag-of-words and tf-idf). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import os\n",
    "import re\n",
    "import warnings\n",
    "\n",
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Text processing\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, ENGLISH_STOP_WORDS\n",
    "\n",
    "# Machine learning\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    cross_val_score,\n",
    "    StratifiedKFold,\n",
    "    GridSearchCV,\n",
    ")\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from scipy.sparse import hstack\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- File Paths Setup ---\n",
    "# Base directory (go 2 levels up from /src1/)\n",
    "base_dir = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "\n",
    "# Define input/output directories using relative paths\n",
    "input_dir = os.path.join(base_dir, \"data\", \"intermediate\")\n",
    "output_dir = os.path.join(base_dir, \"data\", \"intermediate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes\n",
    "#  Suppress warnings\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "\n",
    "# Initialize Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function for Text Preprocessing (Lemmatization & Cleaning)\n",
    "\n",
    "\n",
    "def prepare_text(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r\"[^a-z\\s]\", \"\", text)  # Remove punctuation and special characters\n",
    "    tokens = word_tokenize(text)  # Tokenization\n",
    "    lemmatized_tokens = [\n",
    "        lemmatizer.lemmatize(word) for word in tokens\n",
    "    ]  # Apply lemmatization\n",
    "    return \" \".join(lemmatized_tokens)  # Join back into a string\n",
    "\n",
    "\n",
    "# Load dataset\n",
    "data_file = os.path.join(base_dir, \"data\", \"raw\", \"reviews_2010.json\")\n",
    "df = pd.read_json(data_file, lines=True)\n",
    "\n",
    "# Apply preprocessing to text column\n",
    "df[\"text\"] = df[\"text\"].apply(prepare_text)\n",
    "\n",
    "# Define Multi-Class Sentiment Labels\n",
    "df[\"sentiment\"] = df[\"stars\"].apply(\n",
    "    lambda x: 0 if x <= 2 else (1 if x == 3 else (2 if x == 4 else 3))\n",
    ")\n",
    "\n",
    "# Enhanced TF-IDF representation with expanded stopwords\n",
    "custom_stopwords = list(\n",
    "    set(ENGLISH_STOP_WORDS).union(\n",
    "        {\"great\", \"good\", \"bad\", \"nice\", \"product\", \"service\"}\n",
    "    )\n",
    ")\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=150000,\n",
    "    sublinear_tf=True,\n",
    "    max_df=0.7,\n",
    "    min_df=3,\n",
    "    ngram_range=(1, 2),\n",
    "    stop_words=custom_stopwords,\n",
    "    norm=\"l2\",\n",
    ")\n",
    "X_tfidf = vectorizer.fit_transform(df[\"text\"])\n",
    "\n",
    "# Feature Selection (Optional - Can Be Disabled)\n",
    "USE_CHI2_SELECTION = False  # Set to True if you want to enable feature selection\n",
    "\n",
    "if USE_CHI2_SELECTION:\n",
    "    chi2_selector = SelectKBest(chi2, k=min(25000, X_tfidf.shape[1]))\n",
    "    X_tfidf = chi2_selector.fit_transform(X_tfidf, df[\"sentiment\"])\n",
    "\n",
    "# Feature Engineering - Optimized Selection\n",
    "df[\"review_length\"] = df[\"text\"].apply(lambda x: len(x.split()))\n",
    "df[\"num_exclamation\"] = df[\"text\"].apply(lambda x: x.count(\"!\"))\n",
    "df[\"avg_word_length\"] = df[\"text\"].apply(\n",
    "    lambda x: np.mean([len(word) for word in x.split()]) if x.split() else 0\n",
    ")\n",
    "\n",
    "# Convert additional features to a numpy array\n",
    "X_additional = np.array(df[[\"review_length\", \"num_exclamation\", \"avg_word_length\"]])\n",
    "\n",
    "# Combine TF-IDF with additional features\n",
    "X_final = hstack((X_tfidf, X_additional))\n",
    "y = df[\"sentiment\"].values\n",
    "\n",
    "# Split Data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_final, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# Optimize Naive Bayes Model with Expanded Hyperparameter Tuning\n",
    "param_grid = {\"alpha\": np.linspace(0.001, 2.0, 20)}  # Expanded range for better tuning\n",
    "grid_search = GridSearchCV(\n",
    "    ComplementNB(), param_grid, cv=10, scoring=\"accuracy\", n_jobs=-1\n",
    ")\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_alpha = grid_search.best_params_[\"alpha\"]\n",
    "\n",
    "# Train Optimized Naive Bayes Model\n",
    "nb_classifier = ComplementNB(alpha=best_alpha)\n",
    "nb_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make Predictions\n",
    "y_pred = nb_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate Performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"âœ…  Accuracy: {accuracy:.4f}\")\n",
    "print(\n",
    "    classification_report(\n",
    "        y_test,\n",
    "        y_pred,\n",
    "        target_names=[\"Negative\", \"Neutral\", \"Positive\", \"Very Positive\"],\n",
    "    )\n",
    ")\n",
    "\n",
    "# Perform Stratified Cross-Validation\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "cv_scores = cross_val_score(nb_classifier, X_final, y, cv=skf, n_jobs=-1)\n",
    "print(f\" Stratified Cross-validation accuracy: {cv_scores.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multinomial Logistic Regression algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "file_path = os.path.join(input_dir, \"bow_vector.csv\")\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Assume the first column is the text reviews (not used in ML model) and the second column is \"sentiment label\"\n",
    "X = df.iloc[:, 2:]  # Features (numerical representation of reviews)\n",
    "y = df.iloc[:, 1]  # Target (sentiment label)\n",
    "\n",
    "# Split into train-test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Train Multinomial Logistic Regression\n",
    "model = LogisticRegression(multi_class=\"multinomial\", solver=\"lbfgs\", max_iter=500)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Classification report\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
