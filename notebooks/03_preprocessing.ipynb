{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing\n",
    "This notebook manipulates the review texts. It removes the stopwords, tokenizes each review, then stemms and lemmatizes the tokens. Next with nltk package sentiment of each review is determined. Last a tf-idf and Bag-of-Words algorithm is implemented. The resulting files are saved to data/intermediate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Lucie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Lucie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Lucie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\Lucie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Ensure necessary NLTK resources are downloaded\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# Initialize stemmer, lemmatizer, and sentiment analyzer\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "sia = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- File Paths Setup ---\n",
    "# Base directory (go 2 levels up from /src1/)\n",
    "base_dir = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "\n",
    "# Define input/output directories using relative paths\n",
    "input_dir = os.path.join(base_dir, \"data\", \"intermediate\")\n",
    "output_dir = os.path.join(base_dir, \"data\", \"intermediate\")\n",
    "\n",
    "input_filename = \"cleaned_reviews_2021-01.json\" # Adjust as needed\n",
    "input_file = os.path.join(input_dir, input_filename)\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read JSON file line by line\n",
    "def load_dataset(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        reviews = [json.loads(line) for line in file]\n",
    "    return pd.DataFrame(reviews)\n",
    "\n",
    "# Function for cleaning, tokenizing, and stemming/lemmatizing text\n",
    "def preprocess_text(text):   \n",
    "    # Tokenize text\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # Apply stemming and lemmatization\n",
    "    stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
    "    \n",
    "    return {\n",
    "        \"tokens\": filtered_tokens,\n",
    "        \"stemmed\": stemmed_tokens,\n",
    "        \"lemmatized\": lemmatized_tokens,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment Analysis Function\n",
    "def analyze_sentiment(text):\n",
    "    scores = sia.polarity_scores(text)\n",
    "    if scores[\"compound\"] >= 0.05:\n",
    "        return \"positive\"\n",
    "    elif scores[\"compound\"] <= -0.05:\n",
    "        return \"negative\"\n",
    "    else:\n",
    "        return \"neutral\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF Function and Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF Vectorization function\n",
    "def process_tfidf(df):\n",
    "    # Apply preprocessing and lemmatization\n",
    "    df['processed_text'] = df['text'].apply(preprocess_text)\n",
    "    df['lemmatized_text'] = df['processed_text'].apply(lambda x: ' '.join(x['lemmatized']))\n",
    "    \n",
    "    # Apply optimized TF-IDF Vectorization\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        max_features=5000,  # Limit vocabulary size for efficiency\n",
    "        sublinear_tf=True,  # Scale term frequency logarithmically\n",
    "        max_df=0.95,  # Ignore very common words\n",
    "        min_df=5,  # Ignore very rare words\n",
    "        ngram_range=(1, 2),  # Consider unigrams and bigrams\n",
    "    )\n",
    "    \n",
    "    tfidf_matrix = vectorizer.fit_transform(df['lemmatized_text'])\n",
    "    \n",
    "    # Convert TF-IDF matrix to DataFrame efficiently\n",
    "    tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "    \n",
    "    return df, tfidf_df\n",
    "\n",
    "# Bag-of-Words Vectorization function\n",
    "def process_bag_of_words(df):\n",
    "    # Extract the text from each review\n",
    "    texts = df['text'].tolist()\n",
    "\n",
    "    # Initialize the CountVectorizer\n",
    "    vectorizer = CountVectorizer(stop_words='english', lowercase=True, max_features=1000)\n",
    "\n",
    "    # Fit and transform the texts into BoW vectors\n",
    "    bow_matrix = vectorizer.fit_transform(texts)\n",
    "\n",
    "    # Convert the matrix to an array for easier viewing\n",
    "    bow_array = bow_matrix.toarray()\n",
    "\n",
    "    # Get the feature names (words in the vocabulary)\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "    # Create a DataFrame for the BoW vectors\n",
    "    bow_df = pd.DataFrame(bow_array, columns=feature_names)\n",
    "\n",
    "    return bow_df\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed data to files\n",
    "def save_processed_data(df, tfidf_df, bow_df):\n",
    "    # Save processed reviews\n",
    "    df[['review_id', 'processed_text']].to_json(os.path.join(output_dir, \"processed_reviews_tokens.json\"), orient='records', lines=True)\n",
    "    \n",
    "    # Save lemmatized text\n",
    "    df[['review_id', 'lemmatized_text']].to_json(os.path.join(output_dir, \"lemmatized_text.json\"), orient='records', lines=True)\n",
    "    \n",
    "    # Save TF-IDF scores\n",
    "    tfidf_df.to_csv(os.path.join(output_dir, \"tfidf_scores.csv\"), index=False)\n",
    "    \n",
    "    # Save BoW vectors\n",
    "    bow_df.to_csv(os.path.join(output_dir, \"bow_vectors.csv\"), index=False)\n",
    "\n",
    "    print(\"Processed data saved to files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = load_dataset(input_file)\n",
    "\n",
    "# Apply Sentiment Analysis\n",
    "df['sentiment'] = df['text'].apply(analyze_sentiment)\n",
    "df.to_json(os.path.join(output_dir, \"reviews_with_sentiment.json\"), orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                review_id                                               text  \\\n",
      "0  iBUJvIOkToh2ZECVNq5PDg  ive been eating at this restaurant for over 5 ...   \n",
      "1  HgEofz6qEQqKYPT7YLA34w  how does a delivery person from here get lost ...   \n",
      "2  Kxo5d6EOnOE-vERwQf2a1w  the service is always good the employees are n...   \n",
      "3  STqHwh6xd05bgS6FoAgRqw  two words whipped feta explosion of amazingnes...   \n",
      "4  75Ckhq13s7k-crts_0MY9g  place was great as well as parking food was go...   \n",
      "\n",
      "  sentiment  \n",
      "0  positive  \n",
      "1  positive  \n",
      "2  positive  \n",
      "3  positive  \n",
      "4  positive  \n",
      "    10  10 min  10 minute  10 star  10 year  100  1000  1010  1010 would   11  \\\n",
      "0  0.0     0.0        0.0      0.0      0.0  0.0   0.0   0.0         0.0  0.0   \n",
      "1  0.0     0.0        0.0      0.0      0.0  0.0   0.0   0.0         0.0  0.0   \n",
      "2  0.0     0.0        0.0      0.0      0.0  0.0   0.0   0.0         0.0  0.0   \n",
      "3  0.0     0.0        0.0      0.0      0.0  0.0   0.0   0.0         0.0  0.0   \n",
      "4  0.0     0.0        0.0      0.0      0.0  0.0   0.0   0.0         0.0  0.0   \n",
      "\n",
      "   ...  youre going  youre looking  youve   yr  yum  yummy  zero  zero star  \\\n",
      "0  ...          0.0            0.0    0.0  0.0  0.0    0.0   0.0        0.0   \n",
      "1  ...          0.0            0.0    0.0  0.0  0.0    0.0   0.0        0.0   \n",
      "2  ...          0.0            0.0    0.0  0.0  0.0    0.0   0.0        0.0   \n",
      "3  ...          0.0            0.0    0.0  0.0  0.0    0.0   0.0        0.0   \n",
      "4  ...          0.0            0.0    0.0  0.0  0.0    0.0   0.0        0.0   \n",
      "\n",
      "   zone  zoo  \n",
      "0   0.0  0.0  \n",
      "1   0.0  0.0  \n",
      "2   0.0  0.0  \n",
      "3   0.0  0.0  \n",
      "4   0.0  0.0  \n",
      "\n",
      "[5 rows x 5000 columns]\n"
     ]
    }
   ],
   "source": [
    "# Process TF-IDF (Takes a while to run)\n",
    "processed_df, tfidf_df = process_tfidf(df)\n",
    "print(processed_df[['review_id', 'text', 'sentiment']].head())\n",
    "print(tfidf_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   10  100  1010  12  15  20  2020  2021  25  30  ...  years  yelp  yes  \\\n",
      "0   0    0     0   0   0   0     0     0   0   0  ...      1     0    0   \n",
      "1   0    0     0   0   0   0     0     0   0   0  ...      0     0    0   \n",
      "2   0    0     0   0   0   0     0     0   0   0  ...      0     0    0   \n",
      "3   0    0     0   0   0   0     0     0   0   0  ...      0     0    0   \n",
      "4   0    0     0   0   0   0     0     0   0   0  ...      0     0    0   \n",
      "\n",
      "   yesterday  youll  young  youre  yum  yummy  zero  \n",
      "0          0      0      0      0    0      0     0  \n",
      "1          0      0      0      0    0      0     0  \n",
      "2          0      0      0      0    0      0     0  \n",
      "3          0      0      0      0    0      0     0  \n",
      "4          0      0      0      0    0      0     0  \n",
      "\n",
      "[5 rows x 1000 columns]\n"
     ]
    }
   ],
   "source": [
    "# Process Bag-of-Words\n",
    "bow_df = process_bag_of_words(df)\n",
    "print(bow_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed data saved to files.\n"
     ]
    }
   ],
   "source": [
    "# Save processed data (Also takes a while to run :))\n",
    "save_processed_data(processed_df, tfidf_df, bow_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
